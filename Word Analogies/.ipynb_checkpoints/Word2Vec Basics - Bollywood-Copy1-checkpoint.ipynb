{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Word2Vec Model\n",
    "- Word2Vec Google's Pretrained Model\n",
    "- Contains vector representations of 50 billion words\n",
    "\n",
    "- Words which are similar in context have similar vectors\n",
    "- Distance/Similarity between two words can be measured using Cosine Distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "- Text Similarity\n",
    "- Language Translation\n",
    "- Finding Odd Words\n",
    "- Word Analogies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "- Word embeddings are numerical representation of words, in the form of vectors.\n",
    "\n",
    "- Word2Vec Model represents each word as 300 Dimensional Vector\n",
    "\n",
    "- In this tutorial we are going to see how to use pre-trained word2vec model.\n",
    "- Model size is around 1.5 GB\n",
    "- We will work using Gensim, which is popular NLP Package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim's Word2Vec Model provides optimum implementation of \n",
    "\n",
    "1) **CBOW** Model \n",
    "\n",
    "2) **SkipGram Model**\n",
    "\n",
    "\n",
    "Paper 1 [Efficient Estimation of Word Representations in\n",
    "Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "\n",
    "Paper 2 [Distributed Representations of Words and Phrases and their Compositionality\n",
    "](https://arxiv.org/abs/1310.4546)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using Gensim\n",
    "`Link https://radimrehurek.com/gensim/models/word2vec.html`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**KeyedVectors** - This object essentially contains the mapping between words and embeddings. After training, it can be used directly to query those embeddings in various ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_apple = word_vectors[\"apple\"] \n",
    "v_mango = word_vectors[\"india\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(v_apple.shape)\n",
    "print(v_mango.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17158596]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([v_mango],[v_apple])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Find the Odd One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd_one_out(words):\n",
    "    \"\"\"Accepts a list of words and returns the odd word\"\"\"\n",
    "    \n",
    "    # Generate all word embeddings for the given list\n",
    "    all_word_vectors = [word_vectors[w] for w in words]\n",
    "    avg_vector = np.mean(all_word_vectors,axis=0)\n",
    "    print(avg_vector.shape)\n",
    "    \n",
    "    #Iterate over every word and find similarity\n",
    "    odd_one_out = None\n",
    "    min_similarity = 1.0 #Very high value\n",
    "    \n",
    "    for w in words:\n",
    "        sim = cosine_similarity([word_vectors[w]],[avg_vector])\n",
    "        if sim < min_similarity:\n",
    "            min_similarity = sim\n",
    "            odd_one_out = w\n",
    "    \n",
    "        print(\"Similairy btw %s and avg vector is %.2f\"%(w,sim))\n",
    "            \n",
    "    return odd_one_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = [\"apple\",\"mango\",\"juice\",\"party\",\"orange\"] \n",
    "input_2 = [\"music\",\"dance\",\"sleep\",\"dancer\",\"food\"]        \n",
    "input_3  = [\"match\",\"player\",\"football\",\"cricket\",\"dancer\"]\n",
    "input_4 = [\"india\",\"paris\",\"russia\",\"france\",\"germany\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "Similairy btw apple and avg vector is 0.78\n",
      "Similairy btw mango and avg vector is 0.76\n",
      "Similairy btw juice and avg vector is 0.71\n",
      "Similairy btw party and avg vector is 0.36\n",
      "Similairy btw orange and avg vector is 0.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'party'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_one_out(input_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "Similairy btw music and avg vector is 0.66\n",
      "Similairy btw dance and avg vector is 0.81\n",
      "Similairy btw sleep and avg vector is 0.51\n",
      "Similairy btw dancer and avg vector is 0.72\n",
      "Similairy btw food and avg vector is 0.52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_one_out(input_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "Similairy btw match and avg vector is 0.58\n",
      "Similairy btw player and avg vector is 0.68\n",
      "Similairy btw football and avg vector is 0.72\n",
      "Similairy btw cricket and avg vector is 0.70\n",
      "Similairy btw dancer and avg vector is 0.53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dancer'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_one_out(input_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "Similairy btw india and avg vector is 0.81\n",
      "Similairy btw paris and avg vector is 0.75\n",
      "Similairy btw russia and avg vector is 0.79\n",
      "Similairy btw france and avg vector is 0.81\n",
      "Similairy btw germany and avg vector is 0.84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_one_out(input_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word Analogies Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the word analogy task, we complete the sentence \"a is to b as c is to __\". An example is 'man is to woman as king is to queen' . In detail, we are trying to find a word d, such that the associated word vectors `ea,eb,ec,ed` are related in the following manner: `eb−ea≈ed−ec`. We will measure the similarity between `eb−ea` and `ed−ec` using cosine similarity. \n",
    "\n",
    "![Word2Vec](./images/word2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`man -> woman :: \tprince -> princess`  \n",
    "`italy -> italian :: \tspain -> spanish`  \n",
    "`india -> delhi :: \tjapan -> tokyo`  \n",
    "`man -> woman :: \tboy -> girl`  \n",
    "`small -> smaller :: \tlarge -> larger`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training Your Own Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec model can learn embeddings from any text corpus!\n",
    "- Continuous Bag of Words Model\n",
    "- Skip Gram Model\n",
    "\n",
    "`Algorithm looks at window of target word(Y) to provide context word(X), the model is trained on (X,Y) pairs in a superwised manner.` The algorithm was developed by Tomas Mikolov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Each sentence must be tokenized, into a list of words.\n",
    "\n",
    "- The sentences can be text loaded into memory once,\n",
    "or we can build a data pipeline which iteratively feeds data to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw  = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the file \n",
    "def readFile(file): \n",
    "    f = open(file,'r',encoding='utf-8')\n",
    "    text = f.read()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    data = []\n",
    "    for sent in sentences:\n",
    "        words =  nltk.word_tokenize(sent)\n",
    "        words = [w.lower() for w in words if len(w)>2 and w not in stopw]\n",
    "        data.append(words)\n",
    "        \n",
    "    return data\n",
    "\n",
    "text = readFile('bollywood.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['deepika', 'padukone', 'ranveer', 'singh', 'wedding', 'one', 'biggest', 'bollywood', 'events', 'happened', '2018'], ['the', 'deepika', 'ranveer', 'celebrations', 'hooked', 'phones', 'waiting', 'come', 'also', 'gave', 'enough', 'reason', 'believe', 'stylish', 'two', 'couple'], ['from', 'airport', 'looks', 'reception', 'parties', 'everything', 'entire', 'timeline', 'deepika', 'ranveer', 'wedding', 'style', 'file'], ['not', 'ambanis', 'deepika', 'ranveer', 'priyanka', 'nick'], ['man', 'proves', 'wedding', 'the', 'year', 'this', 'year', 'year', 'big', 'fat', 'lavish', 'extravagant', 'weddings'], ['from', 'isha', 'ambani', 'anand', 'piramal', 'deepika', 'padukone', 'ranveer', 'singh', 'priyanka', 'chopra', 'nick', 'jonas', 'kapil', 'sharma', 'ginni', 'chatrath', '2018', 'saw', 'many', 'grand', 'weddings'], ['but', 'nothing', 'beats', 'man', 'wedding', 'the', 'year', 'award', 'social', 'media'], ['priyanka', 'also', 'shared', 'video', 'featuring', 'nick', 'jonaswas', 'also', 'celebrating', 'the', 'family', 'first', 'celebrated', 'christmas', 'london', 'pictures', 'priyanka', 'chopra', 'nick', 'jonas', 'new', 'year', 'celebrations', 'outstanding'], ['priyanka', 'chopra', 'nick', 'shared', 'glimpses', 'celebration', 'verbier', 'switzerland'], ['priyanka', 'chopra', 'married', 'nick', 'jonas', 'december', 'three', 'wedding', 'receptions', 'one', 'new', 'delhi', 'two', 'mumbai'], ['this', 'year', 'year', 'big', 'fat', 'lavish', 'extravagant', 'weddings'], ['from', 'isha', 'ambani', 'anand', 'piramal', 'deepika', 'padukone', 'ranveer', 'singh', 'priyanka', 'chopra', 'nick', 'jonas', 'kapil', 'sharma', 'ginni', 'chatrath', '2018', 'saw', 'many', 'grand', 'weddings'], ['but', 'nothing', 'beats', 'man', 'wedding', 'the', 'year', 'award', 'social', 'media'], ['kapil', 'sharma', 'ginni', 'chatrath', 'jaggo', 'night', 'december', 'made', 'even', 'special', 'industry', 'friends'], ['kapil', 'sharma', 'ginni', 'chatrath', 'friends', 'long', 'time'], ['there', 'virat', 'side', 'actress', 'wife', 'anushka', 'sharma', 'pleasure', 'audience'], ['while', 'couple', 'rang', 'new', 'year', 'style', 'morning', 'saw', 'virat', 'dress', 'squad', 'attire', 'anushka', 'pink', 'salwar', 'suit'], ['isha', 'ambani', 'married', 'anand', 'piramal', 'year']]\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(text,size=300,window=10,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=116, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deepika', 'padukone', 'ranveer', 'singh', 'wedding', 'one', 'biggest', 'bollywood', 'events', 'happened', '2018', 'the', 'celebrations', 'hooked', 'phones', 'waiting', 'come', 'also', 'gave', 'enough', 'reason', 'believe', 'stylish', 'two', 'couple', 'from', 'airport', 'looks', 'reception', 'parties', 'everything', 'entire', 'timeline', 'style', 'file', 'not', 'ambanis', 'priyanka', 'nick', 'man', 'proves', 'year', 'this', 'big', 'fat', 'lavish', 'extravagant', 'weddings', 'isha', 'ambani', 'anand', 'piramal', 'chopra', 'jonas', 'kapil', 'sharma', 'ginni', 'chatrath', 'saw', 'many', 'grand', 'but', 'nothing', 'beats', 'award', 'social', 'media', 'shared', 'video', 'featuring', 'jonaswas', 'celebrating', 'family', 'first', 'celebrated', 'christmas', 'london', 'pictures', 'new', 'outstanding', 'glimpses', 'celebration', 'verbier', 'switzerland', 'married', 'december', 'three', 'receptions', 'delhi', 'mumbai', 'jaggo', 'night', 'made', 'even', 'special', 'industry', 'friends', 'long', 'time', 'there', 'virat', 'side', 'actress', 'wife', 'anushka', 'pleasure', 'audience', 'while', 'rang', 'morning', 'dress', 'squad', 'attire', 'pink', 'salwar', 'suit']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model[\"deepika\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 300)\n",
      "<built-in function all>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_vectors = [model[w] for w in model.wv.vocab.keys()]\n",
    "all_vectors = np.array(all_vectors)\n",
    "print(all_vectors.shape)\n",
    "print(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = [\"ranveer\",\"deepika\",\"padukone\",\"singh\",\"nick\",\"jonas\",\"chopra\",\"priyanka\",\"virat\",\"anushka\",\"ginni\"]\n",
    "\n",
    "\n",
    "def predict_actor(a,b,c,word_vectors):\n",
    "    \"\"\"Accepts a triad of words, a,b,c and returns d such that a is to b : c is to d\"\"\"\n",
    "    a,b,c = a.lower(),b.lower(),c.lower()\n",
    "    max_similarity = -100 \n",
    "    \n",
    "    d = None\n",
    "    words = actors\n",
    "    \n",
    "    wa,wb,wc = word_vectors[a],word_vectors[b],word_vectors[c]\n",
    "    \n",
    "    #to find d s.t similarity(|b-a|,|d-c|) should be max\n",
    "    \n",
    "    for w in words:\n",
    "        if w in [a,b,c]:\n",
    "            continue\n",
    "        \n",
    "        wv = word_vectors[w]\n",
    "        sim = cosine_similarity([wb-wa],[wv-wc])\n",
    "        \n",
    "        if sim > max_similarity:\n",
    "            max_similarity = sim\n",
    "            d = w\n",
    "    return d    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anushka'"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad = (\"nick\",\"priyanka\",\"virat\")\n",
    "predict_actor(*triad,model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nick'"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad = (\"ranveer\",\"deepika\",\"priyanka\")\n",
    "predict_actor(*triad,model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chopra'"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad = (\"deepika\",\"ranveer\",\"kapil\")\n",
    "predict_actor(*triad,model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'padukone'"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad = (\"ranveer\",\"singh\",\"deepika\")\n",
    "predict_actor(*triad,model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'padukone'"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad1 = (\"priyanka\",\"chopra\",\"deepika\")\n",
    "predict_actor(*triad1,model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chopra'"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad2 = (\"priyanka\",\"jonas\",\"nick\")\n",
    "predict_actor(*triad2,model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](./images/p2.gif)\n",
    "# Well Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
